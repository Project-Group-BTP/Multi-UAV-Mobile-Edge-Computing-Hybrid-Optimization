# MARL Algorithms - Multi-Agent Reinforcement Learning Models

This folder contains 8 different MARL algorithm implementations for multi-agent coordination:

## ğŸ“Š Algorithm Overview

### Base Algorithms (No Attention)

| Algorithm | Type | Best For |
|-----------|------|----------|
| **Random** | None | Random baseline |
| **MADDPG** | Off-policy | Stable baseline |
| **MATD3** | Off-policy | More stable (twin critics) |
| **MAPPO** | On-policy | Fast learning |
| **MASAC** | Off-policy + entropy | Exploration friendly |

### Attention Variants (With Graph Attention Networks)

| Algorithm | Base | Advantage |
|-----------|------|-----------|
| **Attention MADDPG** | MADDPG | Better agent coordination |
| **Attention MATD3** | MATD3 | Stable + coordinated |
| **Attention MAPPO** | MAPPO | Fast + coordinated |
| **Attention MASAC** | MASAC | Exploration + coordination |


## ğŸ“ Folder Structure

```
marl_models/
â”œâ”€â”€ base_model.py          # Base class for all models
â”œâ”€â”€ buffer_and_helpers.py  # Experience replay buffer
â”œâ”€â”€ utils.py               # Model factory & utilities
â”‚
â”œâ”€â”€ maddpg/                # MADDPG implementation
â”‚   â”œâ”€â”€ agents.py          # Agent class
â”‚   â””â”€â”€ maddpg.py          # Algorithm update logic
â”‚
â”œâ”€â”€ matd3/                 # MATD3 implementation
â”œâ”€â”€ mappo/                 # MAPPO implementation
â”œâ”€â”€ masac/                 # MASAC implementation
â”‚
â”œâ”€â”€ attention_maddpg/      # MADDPG + Attention
â”‚   â”œâ”€â”€ agents.py
â”‚   â””â”€â”€ attention_maddpg.py
â”‚
â”œâ”€â”€ attention_matd3/       # MATD3 + Attention
â”œâ”€â”€ attention_mappo/       # MAPPO + Attention
â”œâ”€â”€ attention_masac/       # MASAC + Attention
â”‚
â””â”€â”€ random_baseline/       # Random baseline
    â””â”€â”€ random_model.py
```

## ğŸ”§ How to Use

### Train with Specific Algorithm

```python
# Method 1: Command line
python train.py --model maddpg --episodes 5000

# Method 2: In Python
from train import train_agent
config = get_config()
config.MODEL = "attention_maddpg"
train_agent(config)
```

### Switch Between Algorithms

Simply change `MODEL` in [config.py](../config.py):

```python
# Choose one:
MODEL = "maddpg"              # Off-policy baseline
MODEL = "matd3"               # Twin delayed DDPG
MODEL = "mappo"               # On-policy PPO
MODEL = "masac"               # Soft Actor-Critic
MODEL = "attention_maddpg"    # MADDPG + attention
MODEL = "attention_matd3"     # MATD3 + attention
MODEL = "attention_mappo"     # MAPPO + attention
MODEL = "attention_masac"     # MASAC + attention
MODEL = "random"              # Random baseline
```

## ğŸ¯ Tuning Hyperparameters

### Stage 1: Reward Optimization
No algorithm-specific tuning, just reward weights:
```bash
python tune.py --stage 1 --model maddpg --episodes 500 --trials 50
```

### Stage 2: Algorithm Hyperparameters
Tunes learning rates, network size, batch size, discount factor:
```bash
python tune.py --stage 2 --model maddpg --episodes 1000 --trials 50
```

### Stage 3: Attention Architecture (Attention Models Only)
Optimize attention dimension and heads:
```bash
python tune.py --stage 3 --model attention_maddpg --episodes 500 --trials 30
```

## ğŸ” Comparing Algorithms

```bash
# Train multiple algorithms
python train.py --model maddpg --episodes 5000
python train.py --model masac --episodes 5000
python train.py --model attention_maddpg --episodes 5000

# Compare results
python utils/comparative_plots.py \
  --logs train_logs/maddpg train_logs/masac/ \
  --names MADDPG MASAC \
  --smoothing 10
```

Generates comparison plots showing:
- Reward curves (learning progress)
- Latency (task completion time)
- Energy consumption
- Fairness (equal service)
- Offline rate (battery health)
- Loss curves (training stability)

Refer [Plotting Module](/docs/PLOTTING_MODULE.md) for detailed plotting plan.